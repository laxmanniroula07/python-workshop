{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076c4655-c8aa-4fcb-8a82-98f824aa6729",
   "metadata": {},
   "source": [
    "Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb755a17-8f25-456d-8d5e-3c7b00ebfa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63398c-7558-49f6-8547-fbb9a0f648e4",
   "metadata": {},
   "source": [
    "Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471378f9-37c3-466e-82de-63346f4d9766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1dc396-98eb-4900-b0ca-6721632dbae3",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34290a99-ebe9-468e-b0bb-62814ee17565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead3174-116f-436c-9506-d13da4ade5a2",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae2e130-dbe0-4d9e-a6c8-33174ac34722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "Wake up to reality! Nothing ever goes as planned in this accursed world. The longer you live, the more you realize that the only things that truly exist in this reality are merely pain, suffering and futility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "#Sample text\n",
    "text = \"Wake up to reality! Nothing ever goes as planned in this accursed world. The longer you live, the more you realize that the only things that truly exist in this reality are merely pain, suffering and futility.\"\n",
    "print(\"Original Text: \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525cd6f-024e-4802-acfc-dfd424fe1556",
   "metadata": {},
   "source": [
    "Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e5adef-bc69-417b-8fbe-914ff3830233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower case Text: \n",
      "wake up to reality! nothing ever goes as planned in this accursed world. the longer you live, the more you realize that the only things that truly exist in this reality are merely pain, suffering and futility.\n"
     ]
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "print(\"lower case Text: \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b52c58-756e-4ae2-a873-431fa50ebca2",
   "metadata": {},
   "source": [
    "Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c088ef-12a2-4806-908e-d2f40749191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation : \n",
      "wake up to reality nothing ever goes as planned in this accursed world the longer you live the more you realize that the only things that truly exist in this reality are merely pain suffering and futility\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Removed punctuation : \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fad55d-e341-44f8-b809-a934703687a6",
   "metadata": {},
   "source": [
    "Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d00527-1a25-4475-8a5e-c6afba3efd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens: \n",
      "['wake', 'up', 'to', 'reality', 'nothing', 'ever', 'goes', 'as', 'planned', 'in', 'this', 'accursed', 'world', 'the', 'longer', 'you', 'live', 'the', 'more', 'you', 'realize', 'that', 'the', 'only', 'things', 'that', 'truly', 'exist', 'in', 'this', 'reality', 'are', 'merely', 'pain', 'suffering', 'and', 'futility']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"All tokens: \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c58e4-38da-42e9-a8e4-cdc83e29f151",
   "metadata": {},
   "source": [
    "Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745b8864-1c32-4b6a-b815-ff406b6d1c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Text Preprocessing: \n",
      "['wake', 'reality', 'nothing', 'ever', 'goes', 'planned', 'accursed', 'world', 'longer', 'live', 'realize', 'things', 'truly', 'exist', 'reality', 'merely', 'pain', 'suffering', 'futility']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\nAfter Text Preprocessing: \")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023eac3-4f6d-4771-8376-a1b3b024dbe2",
   "metadata": {},
   "source": [
    "Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f601b262-95fd-4fcf-ab50-a91b071c6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "Once there lived a boy. He was very hardworking. He was very poor\n",
      "lower case Text: \n",
      "once there lived a boy. he was very hardworking. he was very poor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mpunctuation))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved punctuation : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "#Sample text\n",
    "text = \"Once there lived a boy. He was very hardworking. He was very poor\"\n",
    "print(\"Original Text: \")\n",
    "print(text)\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "print(\"lower case Text: \")\n",
    "print(text)\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Removed punctuation : \")\n",
    "print(text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"All tokens: \")\n",
    "print(tokens)\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\nAfter Text Preprocessing: \")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb9790-2ae6-4cab-90ea-48bb99b879c4",
   "metadata": {},
   "source": [
    "Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbb3a74-25da-4099-9c35-f8a96e80aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) is a field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include learning from data, reasoning, problem-solving, understanding natural language, recognizing patterns, and making decisions. AI can be broadly categorized into two types:\n",
      "\n",
      "Narrow AI (Weak AI): This type of AI is designed to perform a specific task, such as speech recognition, image classification, or playing a game. Examples include virtual assistants like Siri and Alexa, recommendation algorithms used by Netflix and Amazon, and autonomous vehicles.\n",
      "\n",
      "General AI (Strong AI): This type of AI aims to possess the ability to understand, learn, and apply intelligence across a wide range of tasks, similar to human intelligence. General AI remains largely theoretical and has not yet been achieved.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'AI.txt'\n",
    "\n",
    "# Read the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c378953-49e4-46c8-a361-38304648c7a5",
   "metadata": {},
   "source": [
    "Exercise 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa50e09-8fcd-4140-8c01-9e513c86aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower case Text: \n",
      "artificial intelligence (ai) is a field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. these tasks include learning from data, reasoning, problem-solving, understanding natural language, recognizing patterns, and making decisions. ai can be broadly categorized into two types:\n",
      "\n",
      "narrow ai (weak ai): this type of ai is designed to perform a specific task, such as speech recognition, image classification, or playing a game. examples include virtual assistants like siri and alexa, recommendation algorithms used by netflix and amazon, and autonomous vehicles.\n",
      "\n",
      "general ai (strong ai): this type of ai aims to possess the ability to understand, learn, and apply intelligence across a wide range of tasks, similar to human intelligence. general ai remains largely theoretical and has not yet been achieved.s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mpunctuation))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved punctuation : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert into lowercase\n",
    "text = text.lower()\n",
    "print(\"lower case Text: \")\n",
    "print(text)\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Removed punctuation : \")\n",
    "print(text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"All tokens: \")\n",
    "print(tokens)\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\nAfter Text Preprocessing: \")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83635572-c7db-40cb-92db-55e6f5300d34",
   "metadata": {},
   "source": [
    "Exercise  12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d406907c-1ef3-46d1-9527-a96bbddf052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: cats, Lemma: cat\n",
      "Word: cacti, Lemma: cactus\n",
      "Word: geese, Lemma: goose\n",
      "Word: rocks, Lemma: rock\n",
      "Word: python, Lemma: python\n",
      "Word: better, Lemma: better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['cats', 'cacti', 'geese', 'rocks', 'python', 'better']\n",
    "for word in words:\n",
    "    print(f\"Word: {word}, Lemma: {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec4854-a15b-477f-9029-fd53c7408933",
   "metadata": {},
   "source": [
    "Exercise 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d02c25-c2fe-4d3f-9178-b2e5bdb658ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Artificial, Lemma: Artificial\n",
      "Word: Intelligence, Lemma: Intelligence\n",
      "Word: (AI), Lemma: (AI)\n",
      "Word: is, Lemma: is\n",
      "Word: a, Lemma: a\n",
      "Word: field, Lemma: field\n",
      "Word: of, Lemma: of\n",
      "Word: computer, Lemma: computer\n",
      "Word: science, Lemma: science\n",
      "Word: focused, Lemma: focused\n",
      "Word: on, Lemma: on\n",
      "Word: creating, Lemma: creating\n",
      "Word: systems, Lemma: system\n",
      "Word: capable, Lemma: capable\n",
      "Word: of, Lemma: of\n",
      "Word: performing, Lemma: performing\n",
      "Word: tasks, Lemma: task\n",
      "Word: that, Lemma: that\n",
      "Word: typically, Lemma: typically\n",
      "Word: require, Lemma: require\n",
      "Word: human, Lemma: human\n",
      "Word: intelligence., Lemma: intelligence.\n",
      "Word: These, Lemma: These\n",
      "Word: tasks, Lemma: task\n",
      "Word: include, Lemma: include\n",
      "Word: learning, Lemma: learning\n",
      "Word: from, Lemma: from\n",
      "Word: data,, Lemma: data,\n",
      "Word: reasoning,, Lemma: reasoning,\n",
      "Word: problem-solving,, Lemma: problem-solving,\n",
      "Word: understanding, Lemma: understanding\n",
      "Word: natural, Lemma: natural\n",
      "Word: language,, Lemma: language,\n",
      "Word: recognizing, Lemma: recognizing\n",
      "Word: patterns,, Lemma: patterns,\n",
      "Word: and, Lemma: and\n",
      "Word: making, Lemma: making\n",
      "Word: decisions., Lemma: decisions.\n",
      "Word: AI, Lemma: AI\n",
      "Word: can, Lemma: can\n",
      "Word: be, Lemma: be\n",
      "Word: broadly, Lemma: broadly\n",
      "Word: categorized, Lemma: categorized\n",
      "Word: into, Lemma: into\n",
      "Word: two, Lemma: two\n",
      "Word: types:, Lemma: types:\n",
      "Word: Narrow, Lemma: Narrow\n",
      "Word: AI, Lemma: AI\n",
      "Word: (Weak, Lemma: (Weak\n",
      "Word: AI):, Lemma: AI):\n",
      "Word: This, Lemma: This\n",
      "Word: type, Lemma: type\n",
      "Word: of, Lemma: of\n",
      "Word: AI, Lemma: AI\n",
      "Word: is, Lemma: is\n",
      "Word: designed, Lemma: designed\n",
      "Word: to, Lemma: to\n",
      "Word: perform, Lemma: perform\n",
      "Word: a, Lemma: a\n",
      "Word: specific, Lemma: specific\n",
      "Word: task,, Lemma: task,\n",
      "Word: such, Lemma: such\n",
      "Word: as, Lemma: a\n",
      "Word: speech, Lemma: speech\n",
      "Word: recognition,, Lemma: recognition,\n",
      "Word: image, Lemma: image\n",
      "Word: classification,, Lemma: classification,\n",
      "Word: or, Lemma: or\n",
      "Word: playing, Lemma: playing\n",
      "Word: a, Lemma: a\n",
      "Word: game., Lemma: game.\n",
      "Word: Examples, Lemma: Examples\n",
      "Word: include, Lemma: include\n",
      "Word: virtual, Lemma: virtual\n",
      "Word: assistants, Lemma: assistant\n",
      "Word: like, Lemma: like\n",
      "Word: Siri, Lemma: Siri\n",
      "Word: and, Lemma: and\n",
      "Word: Alexa,, Lemma: Alexa,\n",
      "Word: recommendation, Lemma: recommendation\n",
      "Word: algorithms, Lemma: algorithm\n",
      "Word: used, Lemma: used\n",
      "Word: by, Lemma: by\n",
      "Word: Netflix, Lemma: Netflix\n",
      "Word: and, Lemma: and\n",
      "Word: Amazon,, Lemma: Amazon,\n",
      "Word: and, Lemma: and\n",
      "Word: autonomous, Lemma: autonomous\n",
      "Word: vehicles., Lemma: vehicles.\n",
      "Word: General, Lemma: General\n",
      "Word: AI, Lemma: AI\n",
      "Word: (Strong, Lemma: (Strong\n",
      "Word: AI):, Lemma: AI):\n",
      "Word: This, Lemma: This\n",
      "Word: type, Lemma: type\n",
      "Word: of, Lemma: of\n",
      "Word: AI, Lemma: AI\n",
      "Word: aims, Lemma: aim\n",
      "Word: to, Lemma: to\n",
      "Word: possess, Lemma: posse\n",
      "Word: the, Lemma: the\n",
      "Word: ability, Lemma: ability\n",
      "Word: to, Lemma: to\n",
      "Word: understand,, Lemma: understand,\n",
      "Word: learn,, Lemma: learn,\n",
      "Word: and, Lemma: and\n",
      "Word: apply, Lemma: apply\n",
      "Word: intelligence, Lemma: intelligence\n",
      "Word: across, Lemma: across\n",
      "Word: a, Lemma: a\n",
      "Word: wide, Lemma: wide\n",
      "Word: range, Lemma: range\n",
      "Word: of, Lemma: of\n",
      "Word: tasks,, Lemma: tasks,\n",
      "Word: similar, Lemma: similar\n",
      "Word: to, Lemma: to\n",
      "Word: human, Lemma: human\n",
      "Word: intelligence., Lemma: intelligence.\n",
      "Word: General, Lemma: General\n",
      "Word: AI, Lemma: AI\n",
      "Word: remains, Lemma: remains\n",
      "Word: largely, Lemma: largely\n",
      "Word: theoretical, Lemma: theoretical\n",
      "Word: and, Lemma: and\n",
      "Word: has, Lemma: ha\n",
      "Word: not, Lemma: not\n",
      "Word: yet, Lemma: yet\n",
      "Word: been, Lemma: been\n",
      "Word: achieved.s, Lemma: achieved.s\n"
     ]
    }
   ],
   "source": [
    "file_path = 'AI.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "words_from_file = text.split()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [(word, lemmatizer.lemmatize(word)) for word in words_from_file]\n",
    "\n",
    "for word, lemma in lemmatized_words:\n",
    "    print(f\"Word: {word}, Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa500ec-43a9-431e-ac0d-2c1414107acb",
   "metadata": {},
   "source": [
    "Exercise 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37afc357-e480-4537-ad1d-45f5175fcb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "This is an example of AI lab Workshop.\n",
      "It demonstrates how i do my  works using python jupyter.\n",
      "Hope you find it helpful.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = \"Hello! This is an example of AI lab Workshop. It demonstrates how i do my  works using python jupyter. Hope you find it helpful.\"\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01711-788e-4359-9985-c96e1641828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
